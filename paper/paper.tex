\documentclass[twoside,11pt,a4paper]{article}

% packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx,curves,float,rotating}

\usepackage{amsmath, amssymb, latexsym}  % math stuff
\usepackage{amsopn}                             % um mathe operatoren zu deklarieren
\usepackage[american]{babel}                     % otherwise use british or american
\usepackage{theorem}                            % instead of \usepackage{amsthm}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}

% @ environment %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xspace}                             % context sensitive space after macros
\makeatletter 
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{{e.g}\onedot} \def\Eg{{E.g}\onedot}
\def\ie{{i.e}\onedot} \def\Ie{{I.e}\onedot}
\def\cf{{c.f}\onedot} \def\Cf{{C.f}\onedot}
\def\etc{{etc}\onedot} \def\vs{{vs}\onedot} 
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{{et al}\onedot}
\def\zB{z.B\onedot} \def\ZB{Z.B\onedot}
\def\dh{d.h\onedot} \def\Dh{D.h\onedot}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%	Macros fuer neue Umgebungen
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\Frac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newlength{\textwd}
\newlength{\oddsidemargintmp}
\newlength{\evensidemargintmp}
\newcommand*{\hspaceof}[2]{\settowidth{\textwd}{#1}\mbox{\hspace{#2\textwd}}}
\newlength{\textht}
\newcommand*{\vspaceof}[3]{\settoheight{\textht}{#1}\mbox{\raisebox{#2\textht}{#3}}}
\newcommand*{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}

\newenvironment{deflist}[1][\quad]%
{  \begin{list}{}{%
      \renewcommand{\makelabel}[1]{\textbf{##1}\hfil}%
      \settowidth{\labelwidth}{\textbf{#1}}%
      \setlength{\leftmargin}{\labelwidth}
      \addtolength{\leftmargin}{\labelsep}}}
{  \end{list}}


\newenvironment{Quote}% Definition of Quote
{  \begin{list}{}{%
      \setlength{\rightmargin}{0pt}}
      \item[]\ignorespaces}
{\unskip\end{list}}


\theoremstyle{break}
\theorembodyfont{\itshape}	
\theoremheaderfont{\scshape}

\newtheorem{Cor}{Corollary}
\newtheorem{Def}{Definition}
%\newtheorem{Def}[Cor]{Definition}



\newcolumntype{.}{D{.}{.}{-1}}


\pagestyle{headings}
\textwidth 15cm
\textheight 23cm
\oddsidemargin 1cm
\evensidemargin 0cm
%\parindent 0mm



\begin{document}


\pagestyle{empty}

\begin{center}

    RWTH Aachen University\\
    Chair of Computer Science 6 \\
    Prof. Dr.-Ing. Hermann Ney\\[6ex]
    Selected Topics in Human Language Technology and Pattern Recognition WS 16/17\\[12ex]
   
    \LARGE
    \textbf{Deep Directed Generative Models} \\[6ex]
    \textit{AndrÃ© Merboldt} \\[6ex]
    \today

    \vfill
    \Large Supervisor: Tobias Menne
	    
\end{center}

% blank page
\newpage
\ 
\newpage


% contents
\pagestyle{headings}
\tableofcontents
\listoftables
\listoffigures
\newpage
\pagestyle{empty}
\ 
\newpage
\pagestyle{headings}


%----------------------------------
% ABSTRACT
%----------------------------------
\section{Abstract}
\label{sec:abstract}
In this seminar paper we provide an overview and introduction to directed generative models,
which are able to produce data following a distribution (...?).
% more info what generative models try to achieve
After we have introduced several architectures, we will explore sigmoid belief networks (SBN)
as an early generative model.\\

Then we will turn to more recent proposals, in particular variational autoencoders (VAE) and
generative adversarial networks (GAN).
Variants of GANs have been shown to produce images which are able to fool a human discriminator 40\% of the time (citation needed, LAPGAN? or DCGAN?).
Additionally to exploring these models and theoretical foundation, we provide an intuitive and simple application of a GAN.






%----------------------------------
% Motivation
%----------------------------------
\section{Motivation}
\label{sec:motivation}
Generative models have been proposed as one of the most promising approaches towards
learning representations of real world data by some of the leading researches (YannLeCun, OpenAI blogpost).

In recent years supervised learning has yielded impressive results,
however for these models to suceed huge amount of labelled data is needed.
Unsupervised learning, that is learning from unlabelled data, has been expressed
as one hurdles toward general articial intelligence (citation needed).
Early approaches toward this problem however have shown that these problems are very hard to learn (intractable?).

In generative models the approach is different in a way that
we search for a good internal representation of the data.
This resembles the way humans learn about the world where we incrementally build
a model of the world.


%----------------------------------
% Overview of Directed Generative Models
%----------------------------------
\section{Overview of Directed Generative Models}
\label{sec:overview}


\paragraph{Variational Autoencoders\cite{vae:2013}}
\label{par:overview_vae}
Autoencoders comprise of two components, the encoder and decoder
where the encoder takes input data and translates it into a latent structure while
the decoder takes the latent structure and produces data which is as close as possible to the initial input data.

In the case of VAEs, the distribution is intractable and we can only approximate the posterior distribution.
% intractable -> use neural network, which we can train
% --> no closed form possible
We use the KL-Divergence to train the network.

% TODO read chapter 19, approximate inference
In VAEs however, this 




\paragraph{Generative Adversarial Networks}
\label{par:overview_gan}
GANs are a game theoretic approach to generative modelling
in which two networks compete against each other.
The discriminator $D$ is trying to distinguish the generated samples
by the generator $G$ from real world data.
The goal for $G$ however is to produce data as realistic as possible.\\
During the early phase of training, $G$ produces random noise which
is easy to differentiate from groundtruth data.
Both networks are trained in parallel, where both networks try to get better
at their respective objective.
In the best-case scenario is $G$ able to produce data indistinguishly from real world data in which
case the discriminator will only be able to guess correctly with a 50\% chance.
This plateau is called Nash equilibrium (citation needed).






%----------------------------------
% Sigmoid Belief Networks
%----------------------------------
\section{Sigmoid Belief Networks}
\label{sec:sbn}


%----------------------------------
% Variational Autoencoders
%----------------------------------
\section{Variational Autoencoders}
\label{sec:vae}

\subsection{Architecture}
\label{sub:vae_architecture}

\subsection{Extensions}
\label{sub:vae_extensions}

\subsubsection{Deep Recurrent Attention Writer (DRAW)}
\label{ssub:deep_recurrent_attention_writer_draw_}








%----------------------------------
% Generative Adversarial Netoworks
%----------------------------------
\section{Generative Adversarial Networks}
\label{sec:gan}

\subsection{Architecture}
\label{sub:gan_arch}

\subsection{Training}
\label{sub:gan_training}

\subsection{Stability and Performance}
\label{sub:gan_stability}

\subsubsection{Freeze Learning}
\label{ssub:gan_freeze_learning}

\subsubsection{Feature Matching}
\label{ssub:gan_feature_matching}

\subsubsection{Minibatch discrimination}
\label{ssub:gan_minibatch_discrimination}

\subsubsection{Historical Averaging}
\label{ssub:gan_historical_averaging}


\subsubsection{Label Smoothing}
\label{ssub:gan_label_smoothing}











\subsection{Application}
\label{sub:gan_application}

\subsection{Extensions}
\label{sub:gan_extensions}

\subsubsection{LAPGAN}
\label{ssub:lapgan}

\subsubsection{DCGAN}
\label{ssub:dcgan}





%----------------------------------
% More Generative Directed Models
%----------------------------------
\section{More Generative Directed Models}
\label{sec:more}

\subsection{Generative Moment Matching Networks}
\label{sub:more_mmn}

\subsection{Auto-Regressive Networks}
\label{sub:more_arn}

See \cite{gan_openai:2016} and \cite{gan_training:2016}.


\section{Conclusion}
\label{sec:conclusion}

Generative models have shown great promise to tackle interesting problems.
Academic work is moving incredibly fast and many new ideas and combinations
are being proposed almost daily (citation needed, meta paper?).
For example, VAEs and GANs have been combined (https://github.com/skaae/vaeblog)
which produced way larger images than GANs alone.

% not relevant, but wanted to write about it :p
As mentioned in the motivation, further advances to generative architectures are
likely to solve current issues with depending on large labelled data.
Another large and increasing area of research is semi-supervised learning
(combining learning from labelled data as well as unlabelled data)
and also one-shot learning, which is where humans excel and machines fail currently.
\\\\
Exciting times :)

\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{paper}

\end{document}
