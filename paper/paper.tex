\documentclass[twoside,11pt,a4paper]{article}

% packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx,curves,float,rotating}

\usepackage{amsmath, amssymb, latexsym}  % math stuff
\usepackage{amsopn}                             % um mathe operatoren zu deklarieren
\usepackage[american]{babel}                     % otherwise use british or american
\usepackage{theorem}                            % instead of \usepackage{amsthm}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}

% @ environment %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xspace}                             % context sensitive space after macros
\makeatletter 
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{{e.g}\onedot} \def\Eg{{E.g}\onedot}
\def\ie{{i.e}\onedot} \def\Ie{{I.e}\onedot}
\def\cf{{c.f}\onedot} \def\Cf{{C.f}\onedot}
\def\etc{{etc}\onedot} \def\vs{{vs}\onedot} 
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{{et al}\onedot}
\def\zB{z.B\onedot} \def\ZB{Z.B\onedot}
\def\dh{d.h\onedot} \def\Dh{D.h\onedot}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%	Macros fuer neue Umgebungen
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\Frac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newlength{\textwd}
\newlength{\oddsidemargintmp}
\newlength{\evensidemargintmp}
\newcommand*{\hspaceof}[2]{\settowidth{\textwd}{#1}\mbox{\hspace{#2\textwd}}}
\newlength{\textht}
\newcommand*{\vspaceof}[3]{\settoheight{\textht}{#1}\mbox{\raisebox{#2\textht}{#3}}}
\newcommand*{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}

\newenvironment{deflist}[1][\quad]%
{  \begin{list}{}{%
      \renewcommand{\makelabel}[1]{\textbf{##1}\hfil}%
      \settowidth{\labelwidth}{\textbf{#1}}%
      \setlength{\leftmargin}{\labelwidth}
      \addtolength{\leftmargin}{\labelsep}}}
{  \end{list}}


\newenvironment{Quote}% Definition of Quote
{  \begin{list}{}{%
      \setlength{\rightmargin}{0pt}}
      \item[]\ignorespaces}
{\unskip\end{list}}


\theoremstyle{break}
\theorembodyfont{\itshape}	
\theoremheaderfont{\scshape}

\newtheorem{Cor}{Corollary}
\newtheorem{Def}{Definition}
%\newtheorem{Def}[Cor]{Definition}



\newcolumntype{.}{D{.}{.}{-1}}


\pagestyle{headings}
\textwidth 15cm
\textheight 23cm
\oddsidemargin 1cm
\evensidemargin 0cm
%\parindent 0mm



\begin{document}


\pagestyle{empty}

\begin{center}

    RWTH Aachen University\\
    Chair of Computer Science 6 \\
    Prof. Dr.-Ing. Hermann Ney\\[6ex]
    Selected Topics in Human Language Technology and Pattern Recognition WS 16/17\\[12ex]
   
    \LARGE
    \textbf{Deep Directed Generative Models} \\[6ex]
    \textit{AndrÃ© Merboldt} \\[6ex]
    \today

    \vfill
    \Large Supervisor: Tobias Menne
	    
\end{center}

% blank page
\newpage
\ 
\newpage


% contents
\pagestyle{headings}
\tableofcontents
\listoftables
\listoffigures
\newpage
\pagestyle{empty}
\ 
\newpage
\pagestyle{headings}


%----------------------------------
% ABSTRACT
%----------------------------------
\section{Abstract}
\label{sec:abstract}
In this seminar paper we provide an overview and introduction to directed generative models,
which are able to produce data following a distribution (...?).
% more info what generative models try to achieve
After we have introduced several architectures, we will explore sigmoid belief networks (SBN)
as an early generative model.\\

Then we will turn to more recent proposals, in particular variational autoencoders (VAE) and
generative adversarial networks (GAN).
Variants of GANs have been shown to produce images which are able to fool a human discriminator 40\% of the time (citation needed, LAPGAN? or DCGAN?).
Additionally to exploring these models and theoretical foundation, we provide an intuitive and simple application of a GAN.






%----------------------------------
% Motivation
%----------------------------------
\section{Motivation}
\label{sec:motivation}
Generative models have been proposed as one of the most promising approaches towards
learning representations of real world data by some of the leading researches (YannLeCun, OpenAI blogpost).

In recent years supervised learning has yielded impressive results,
however for these models to suceed huge amount of labelled data is needed.
Unsupervised learning, that is learning from unlabelled data, has been expressed
as one hurdles toward general articial intelligence (citation needed).
Early approaches toward this problem however have shown that these problems are very hard to learn (intractable?).

In generative models the approach is different in a way that
we search for a good internal representation of the data.
This resembles the way humans learn about the world where we incrementally build
a model of the world.


%----------------------------------
% Overview of Directed Generative Models
%----------------------------------
\section{Overview of Directed Generative Models}
\label{sec:overview}


\paragraph{Variational Autoencoders\cite{vae:2013}}
\label{par:overview_vae}
Autoencoders comprise of two components, the encoder and decoder
where the encoder takes input data and translates it into a latent structure while
the decoder takes the latent structure and produces data which is as close as possible to the initial input data.

In the case of VAEs, the distribution is intractable and we can only approximate the posterior distribution.
% intractable -> use neural network, which we can train
% --> no closed form possible
We use the KL-Divergence to train the network.

% TODO read chapter 19, approximate inference
In VAEs however, this 




\paragraph{Generative Adversarial Networks}
\label{par:overview_gan}
GANs are a game theoretic approach to generative modelling
in which two networks compete against each other.
The discriminator $D$ is trying to distinguish the generated samples
by the generator $G$ from real world data.
The goal for $G$ however is to produce data as realistic as possible.\\
During the early phase of training, $G$ produces random noise which
is easy to differentiate from groundtruth data.
Both networks are trained in parallel, where both networks try to get better
at their respective objective.
In the best-case scenario is $G$ able to produce data indistinguishly from real world data in which
case the discriminator will only be able to guess correctly with a 50\% chance.
This plateau is called Nash equilibrium (citation needed).






%----------------------------------
% Sigmoid Belief Networks
%----------------------------------
\section{Sigmoid Belief Networks}
\label{sec:sbn}


%----------------------------------
% Variational Autoencoders
%----------------------------------
\section{Variational Autoencoders}
\label{sec:vae}

%Variational Autoencoders (VAE) have been a popular choice for unsupervised learning of complicated distributions (citation needed) and generative modelling.
In order to generate data from unknown and mostly intractable distributions, we need approximations.
There are basically two approaches for sampling from these distributions,
first there are approximate samples (MCMC, gibbs sampling, etc) which try to directly approximate $p(x)$.
Variational Autoencoders instead try to match an easier to compute distribution $q(x)$ to $p(x)$.
By using this approach, VAEs are computationally less intensive (citation?) but have the drawback of being more restricted in their modelling approach.
Practically, this means that with more computation MCMC methods approach $p(x)$, while there is no such guarantee for variational methods (--> not for all).

VAE can be learned with just backpropagation (paper,TODO), but they differ from denoising and sparse autoencoders due to the different loss function.


%VAEs are built on top of neural networks and are designed in a way to allow training with gradient-based methods.
%Learning and inference are reasonable efficient and relatively easy to implement and show decent results, but have been overshadowed by more recent adversarial approaches (citation needed!!,see \ref{sec:gan}).

\subsection{Architecture}
\label{sub:vae_architecture}
VAE just like other autoencoders encode the input data into a smaller latent space similar to compression of data and is able to decode a vector of latent variables into output while trying to match the output to the input.
But in contrast to other autoencoders (sparse, denoising), we enforce a specific distribution on the latent space.
This allows to sample from this distribution and generate output which will look similar to the data on which the VAE has been trained.
\ref{fig:vae_architecture}.

\subsubsection{Reparameterization Trick}
To be able to backpropagate the loss function through the VAE, it has to be differentiable and deterministic.
Because we add noise to the encoding, the gradients can't be computed directly. In order to circumvent this restriction, the so-called "reparameterization trick" is applied.
Instead of drawing $z ~ \mathcal{N}(\mu(x), \Sigma(x))$, we sample an auxiliary variable $\epsilon$ from $\mathcal{N}(0, I)$ which we then transform with equation \ref{eq:rep_trick} into $z$.
\begin{equation}
  \label{eq:rep_trick}
  z = \mu(x) + \Sigma^{1/2}(x)*\epsilon
\end{equation}
This allows us to compute the gradient of the loss function and backpropagate through the entire model and only have the stochastic variable $\epsilon$ as input.\\\\

The core idea of VAEs is to match a distribution $q(x)$ to the desired $p(x)$ by enforcing a lower variational bound on $q(x)$ in a way that it seeks to match $p(x)$.
In contrast to other autoencoders, VAEs behave differently due to this lower bound but they resemble the architecture of traditional autoencoders (sparse, denoising).



Variational Autoencoders achieve this by using a measurement of inequality between $q(x)$ and $p(x)$ called Kullback-Leibler Divergence (KL divergence).

%VAEs are unlike \emph{traditional} auto-encoders (sparse autoencoders or denoising autoencoders for example) because they have a different architecture
%VAEs resembles traditional autoencoders
%\begin{equation}
%P(x) = \int P(x|z;\theta)P(z) dz
  %\label{eq:max_likelihood}
%\end{equation}

We use VAEs when we have a complicated distribution $p_\theta(x,z)$ with unknown latent variables $z$.
The prior distribution $p_\theta(z)$ over the latent structure is a centered isotropic(?) multivariate Gaussian denoted by $\mathcal{N}(z;0, I)$.
The constructed architecture uses an probabilistic encoder $q_\theta(z|x)$ and a probabilistic decoder $p_\theta(x|z)$ in a form of neural networks, in the original paper MLP are used\cite{vae:2013} but there are various extensions (see \ref{sub:vae_extensions}, TODO).
Because the posterior is intractable ($p_\theta(x)$), we instead approximately maximize the lower variational bound $L(\theta,\phi;x)$.\\
\begin{equation}
  \mathcal{L}(\theta,\phi;x) = -D_{KL}(q_\theta(z|x)||p_\theta(z)) + \mathbb{E}_{q_\theta(z|x)}[log_{p_\theta}(x|z)]
\end{equation}
\paragraph{Derivation of lower bound}
We want to minimize the KL divergence from $P(z|x)$ to $Q(z)$ for some arbitrary $Q$.
  \begin{align*}
    \label{eq:der_lower_bound}
    \mathcal{D}[Q(z)||P(z|x)] &= \mathbb{E}_{z~q_\theta}(\log Q(z) - \log P(z|x))
    &= \mathbb{E}_{z ~ q_\theta}[]
  \end{align*}
Where $D_{KL}(q_\theta(z|x) || p_\theta(z))$ is defined as the Kullback-Leibler divergence, which can be intuitively thought of as a measurement for the distance between two probabilistic distributions (even though it lacks the symmetric property).





\subsection{Learning}
\label{sub:vae_learning}
In practice, the usual choice for $q_\theta(z|x)$ is the multivariate gaussian distribution.
$$
q_\theta(z|x) = \mathcal{N}(z|\mu(X;\vartheta), \Sigma(X;\vartheta))
$$
Because we assume that $P(z)$ is also a multivariate Gaussian distribution, the KL-divergence can now be computed in closed form as follows\cite{derivations:2007}.
\begin{align*}
  \mathcal{D}_{\mathrm{KL}}\big[Q(z) || P(z|x)\big] &= \mathbb{E}\big[\log \frac{p}{q}\big]\\
  \mathcal{D}_{\mathrm{KL}}\big[\mathcal{N}(\mu_0,\Sigma_0) || \mathcal{N}(\mu_1,\Sigma_1)\big]
  &= \frac{1}{2} \big(\mathrm{tr}\big(\Sigma_1^{-1}\Sigma_0\big) + \big(\big)\big)
\end{align*}


\subsection{Inference}
\label{sub:vae_inference}

\subsection{Performance}
\label{sub:vae_performance}

\subsection{Extensions}
\label{sub:vae_extensions}

\subsubsection{Deep Recurrent Attention Writer}
\label{ssub:vae_deep_recurrent_attention_writer}

\subsubsection{Importance Weighted Autoencoder}
\label{ssub:vae_importance_weighted_autoencoder}

\subsubsection{Conditional VAE}
\label{ssub:vae_conditional_vae}









%----------------------------------
% Generative Adversarial Netoworks
%----------------------------------
\section{Generative Adversarial Networks}
\label{sec:gan}

Generative Adversarial Networks (GAN) are a recently proposed framework by Goodfellow et al which has gathered a lot of attention in the deep learning community and in the last few years many extensions have been [proposed] (citations).
GANs can be understood as a two-player game where one player is called generator $G$ playing against the discriminator $D$.
The goal of $G$ is to produce similar data to the given training set while $D$ is tasked with discriminating(?) the generated samples from the training data. Both $G$ and $D$ are learning function approximators, for example Multi-Layer Peceptron (MLP) networks as proposed in the original GAN paper.
% --> declare distribution p_data, p_model, x, z
During learning, both networks are updated in parallel according to the gradient of their respective loss function which we will explore now.\\
In the following $p_{data}$ is the true data generating distribution from which the training samples have been generated.
We don't have access to that distribution.

\subsection{Architecture}
\label{sub:gan_arch}

\subsubsection{Generator Network}
The generator network in the GAN framework tries to fool the discriminator $D$ by producing samples which are indistinguishable from the training data.


\subsubsection{Discriminator Network}


\subsection{Training}
\label{sub:gan_training}

\subsection{Stability and Performance}
\label{sub:gan_stability}

\subsubsection{Freeze Learning}
\label{ssub:gan_freeze_learning}

\subsubsection{Feature Matching}
\label{ssub:gan_feature_matching}

\subsubsection{Minibatch discrimination}
\label{ssub:gan_minibatch_discrimination}

\subsubsection{Historical Averaging}
\label{ssub:gan_historical_averaging}


\subsubsection{Label Smoothing}
\label{ssub:gan_label_smoothing}











\subsection{Application}
\label{sub:gan_application}

\subsection{Extensions}
\label{sub:gan_extensions}

\subsubsection{LAPGAN}
\label{ssub:lapgan}

\subsubsection{DCGAN}
\label{ssub:dcgan}





%----------------------------------
% More Generative Directed Models
%----------------------------------
\section{More Generative Directed Models}
\label{sec:more}

\subsection{Generative Moment Matching Networks}
\label{sub:more_mmn}

\subsection{Auto-Regressive Networks}
\label{sub:more_arn}

See \cite{gan_openai:2016} and \cite{gan_training:2016}.


\section{Conclusion}
\label{sec:conclusion}

Generative models have shown great promise to tackle interesting problems.
Academic work is moving incredibly fast and many new ideas and combinations
are being proposed almost daily (citation needed, meta paper?).
For example, VAEs and GANs have been combined (https://github.com/skaae/vaeblog)
which produced way larger images than GANs alone.

% not relevant, but wanted to write about it :p
As mentioned in the motivation, further advances to generative architectures are
likely to solve current issues with depending on large labelled data.
Another large and increasing area of research is semi-supervised learning
(combining learning from labelled data as well as unlabelled data)
and also one-shot learning, which is where humans excel and machines fail currently.
\\\\
Exciting times :)

\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{paper}

\end{document}
