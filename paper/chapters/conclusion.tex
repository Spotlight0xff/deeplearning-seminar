\section{Summary}
\label{sec:summary}

In this article, we have taken a look at various directed models which perform generative modeling using deep neural networks.\\
After a short overview over traditional generative models and models not covered in this article, we have taken a look at a wide area of applications using generative models.\\\\
%
As one of the first basic generative models, the sigmoid belief net lays the foundation for directed generative models and displays its weaknesses during inference and training.\\
The variational autoencoders have been discussed in more detail using variational inference to turn the inference problem into an optimization problem and solving it there.
VAE and its various extensions have been shown to be a flexible tool applicable for different tasks.\\
%
Generative adversarial networks introduced in Section~\ref{sec:gan} uses a game-theoretic approach to generative modeling. In recent years, GANs have been extended of which we discussed a subset.\\\\
%
Deep generative models have shown the capability to learn rich internal representation of high-dimensional data in order to generate samples using recent advances in deep learning.
Going further, as intelligent agents will need to have accurate and generalized internal representations inferred through sensory input to reason about the world, generative models are likely to be the cornerstones of more advanced intelligent systems in the future.

%Both models explained in detail have been proposed in recent years with many proposed extensions which were not covered in this article, thus further improvements are likely to happen.

Especially interesting is the case of unsupervised learning, due to vast amount of freely available data on the web and in images, video and audio. Deep learning has been mostly focussed on supervised algorithms, however these methods require most labelled datasets which are often not feasible to obtain.

%Generative models have shown great promise to tackle interesting problems.
%Academic work is moving incredibly fast and many new ideas and combinations
%are being proposed almost daily (citation needed, meta paper?).
%For example, VAEs and GANs have been combined (https://github.com/skaae/vaeblog)
%which produced way larger images than GANs alone.

%% not relevant, but wanted to write about it :p
%As mentioned in the motivation, further advances to generative architectures are
%likely to solve current issues with depending on large labelled data.
%Another large and increasing area of research is semi-supervised learning
%(combining learning from labelled data as well as unlabelled data)
%and also one-shot learning, which is where humans excel and machines fail currently.
%\\\\
%Exciting times :)


