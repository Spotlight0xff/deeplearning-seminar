% write about multiple stochastic layers

% write about k passes (just averages)

% write about IWAE, extends these k passes with importance weights
Importance Weighted Autoencoders (IWAE) extends on the VAE framework using $k$-sample importance weighting of the log-likelihood.
% add intuition for IWAE, ruslan explained it very well during his DL summer school talk! (talk_Montreal_2016_Salakhutdinov.pdf)
Using importance weighting can be informally interpreted as putting more weight on data with high likelihood.

The authors of the papers argue that using more samples will only tighten the lower variational bound.
$$
\log p(x) \geq \mathcal{L}_{k+1}(x) \geq \mathcal{L}_{k}(x)
$$

Additionally, if $\frac{p(h,x)}{q(h|x)}$ is bounded, then the model is guaranteed to reach the optimimum:
$$
\lim_{k \rightarrow \infty} \mathcal{L}_{k}(x) = \log p(x)
$$

Of course, using more samples will result in higher computational cost.

%\vspace{5cm}
