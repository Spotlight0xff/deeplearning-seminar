\section{Abstract}
\label{sec:abstract}
%One of the key aspects of human intelligence is the ability to construct accurate and generalized internal representations by observing high-dimensional data.
%We present several deep directed generative models aiming to learn rich and hierarchical latent structures, either implicitly or explicitly, as well as transforming them back into input space.

Deep directed generative models leverage the possibilities of deep learning to learn rich, hierarchical structures from datasets in a possibly unsupervised context.
These models can be used to understand high-dimensional, real-world data and transform them into compact representations. Similar to how humans learn to generalize from sensory input, we discuss several models which can learn compact, internal representational structures.
In this article, we will focus on two recently proposed architectures, variational autoencoders, and generative adversarial networks.


% maybe include:
% * unsupervised models are in high demand due to the massive increase in the amount of unlabelled data due to web, videos, internat in general, laboratory measurements, etc.


%Deep Learning has advanced at an incredible speed the last few years and has achieved human-level performance in some discrimination and classification tasks (object recognition, speech recognition).
%These supervised methods have the huge drawback of requiring a vast amount of labelled data which is often expensive or unfeasible to obtain.
%Learning hidden structure from unlabelled data known as unsupervised learning has had less of an impact recently.
%However, it can be argued that this method is most similar to human learning, in particular inferring information from observations and building an appropriate model from it.
%We will discuss and explore some models for performing this inference as well as generating new data from these representations.
%Building reasonable representation of the world from unlabelled data is arguably one of the key aspects of machine intelligence.
%However one of the key aspects to machine intelligence is learning a rich (?) model about the world and draw conclusions from it.
%The above mentioned tasks require huge amount of labelled data to perform well, which is one of the restrictions of these supervised learning mechanisms.
%Unsupervised learning has the potential to make use of the vast unlabelled data surrounding the world.
%This data can be used to learn representations


%In this seminar paper we provide an overview and introduction to directed generative models,
%which are able to produce data following a distribution (...?).
% more info what generative models try to achieve
%After we have introduced several architectures, we will explore sigmoid belief networks (SBN)
%as an early generative model.\\

%Then we will turn to more recent proposals, in particular variational autoencoders (VAE) and
%generative adversarial networks (GAN).
%Variants of GANs have been shown to produce images which are able to fool a human discriminator 40\% of the time (citation needed, LAPGAN? or DCGAN?).
%Additionally to exploring these models and theoretical foundation, we provide an intuitive and simple application of a GAN.


