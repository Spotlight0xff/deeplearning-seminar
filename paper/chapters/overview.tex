
\newpage

\section{Overview of Directed Generative Models}
\label{sec:overview}

\subsection{Sigmoid Belief Nets}
\label{par:overview_sbn}
Sigmoid Belief Nets (SBN) are a specific type of bayesian networks consisting of an set of binary states with each state depending on activations of preceeding nodes.


Bayesian networks or belief networks are an probabilistic interpretation of an directed acyclic graphical model with a set of states $s$.
As usual in the bayesian framework, each state can be interpreted as a random variable with their conditional probabilities.
%SBNs are one of the earliest deep directed generative models, consisting of binary states $s$ most commonly divided into many layers.

%where in the most common form the architecture consists of several layers where each layer $h_i$ is connected from the previous layer $h_{i-1}$.
%Sampling is an easy task in this model, however inferring good structure for the first layer is a difficult task.
%Usually, in the context of probabilistic generative models we are mostly interested in three different aspects.
%First, sampling from the network. This means given 
%We are mostly interested in three different things involving

%\vspace{3cm}
%using a simple architecture in which sampling from is easy, but learning and performing inference is not.

\subsection{Differentiable Generator Networks}
Most of recent success in deep learning can be attributed to the backpropagation algorithm, including several useful extensions like dropout (citation), or piecewise-linear activation units providing a well-behaving gradient.
Most prominent examples of recent directed generative models leverage these powerful gradient-based optimization techniques by including a generator network.

Most prominent examples of models with differentiable generator nets are the variational autoencoder (VAE) \cite{vae:2014} and generative adversarial networks (GAN) \cite{gan:2014}, which we will discuss in detail later in \ref{sec:vae} and \ref{sec:gan} respectively.


%Due to the success of gradient-based methods for learning deep neural networks, a large part of recent models are using a differentiable generator network allowing error backpropagation.\\

%Both of these models contain a generator network coupled with another network which enables the models to be learned end-to-end.
Both models include generator networks representing a family of parametrized non-linear functions $g_\varphi$ parametrized by $\varphi$ where these parameters can be learned by gradient-based techniques.\\
As neural networks are basically powerful function approximators, they can also be understood as a distribution transformation.
Given samples $z$ from one distribution $p(z)$, applying $g$ is equivalent to sampling from the posterior distribution $p(x|z)$ which is generally more complicated than the prior $p(z)$.\\


%which transforms samples $z$ from a distribution $p(z)$ into $x$ values from a different, possible more complicated distribution $p(x|z)$.
The advantage of this approach is that powerful gradient-based optimization techniques can be used to learn the transformation $g$ without modifying the prior distribution $p(z)$.
The parameters of $g$ can be learned using training data and an appropiate loss function.\\
\newpage
%However, instead of maximizing the log-likelihood of $p(x)$ directly we choose $g$ to emit not samples of $x$ but parameters of a conditional distribution over $x$.

%\subsection{Auto-regressive networks}

%\paragraph{Variational Autoencoders (VAE) \cite{vae:2013}\cite{rezende:2014}}
%\label{par:overview_vae}
%Variational Autoencoders (VAE) use two networks which are trained jointly using approximated learned inference.
%The objective function is aimed at building a reasonable simple latent space and approximating the intractable posterior distribution using variational inference.
%It consists of the recognition and inference network which can be understood in the autoencoder jargon as probabilistic encoder and decoder.
%Using variational bayesian methods the intractable posterior distribution can be approximated and learned using backpropagation and gradient-based methods.

%But in addition to regular autoencoders encourages the VAE the latent space to match a specific probability distribution.
%This encouragement is done using a variational lower bound on the distribution output by the encoder network.

%where the encoder takes input data and translates it into a latent structure while
%the decoder takes the latent structure and produces data which is as close as possible to the initial input data.

%In the case of VAEs, the distribution is intractable and we can only approximate the posterior distribution.
% intractable -> use neural network, which we can train
% --> no closed form possible
%We use the KL-Divergence to train the network.

% TODO read chapter 19, approximate inference
%In VAEs however, this




%\paragraph{Generative Adversarial Networks\cite{gan:2014}}
%\label{par:overview_gan}
%GANs are a game theoretic approach to generative modelling
%in which two networks compete against each other.
%The discriminator $D$ is trying to distinguish the generated samples
%by the generator $G$ from real world data.
%The goal for $G$ however is to produce data as realistic as possible.\\
%During the early phase of training, $G$ produces random noise which
%is easy to differentiate from groundtruth data.
%Both networks are trained in parallel, where both networks try to get better
%at their respective objective.
%In the best-case scenario is $G$ able to produce data indistinguishly from real world data in which
%case the discriminator will only be able to guess correctly with a 50\% chance.
%This plateau is called Nash equilibrium (citation needed).




